{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather ETL\n",
    "\n",
    "![bttf image](bttflogo.png)\n",
    "\n",
    "\n",
    "\n",
    "Data is being collected to understand what were the weather conditions during a shipment and how those conditions influence the fuel consumption. \n",
    "\n",
    "In order to showcase my interpretation of the solution the transformation is dissected and showing all the outputs for the specific cell.\n",
    "\n",
    "In this case the weather data arrives into a landing zone, so in order to reach it let's glob the files:\n",
    "\n",
    "*Globbing this data takes about 50 secs and the current storage use of the landing zone is 2.26 MB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# getting the json files names from the landing zone and appending into the data dict\n",
    "globbed_files = glob.glob(r\"C:/Users/E.ALVAREZHERNANDEZ/VS Projects/caseStudy/landing_zone/weather/*.json\")\n",
    "\n",
    "data = [] \n",
    "for file in globbed_files:\n",
    "    frame = pd.read_json(file, lines=True)\n",
    "    data.append(frame)\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating everything into a single dataframe\n",
    "\n",
    "df = pd.concat(data, ignore_index=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Flattening: \n",
    "\n",
    "Our first challenge is to flatten the data, since relevant values are in a struct form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First I normalize the first level of the wind column\n",
    "from pandas import json_normalize\n",
    "\n",
    "\n",
    "df = df.join(json_normalize(df['weather'].to_list()))\\\n",
    "       .drop(['weather'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize to enter into the second level\n",
    "df = df.join(json_normalize(df['wind'].to_list()))\\\n",
    "       .drop(['wind'], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting into the wind information\n",
    "df = df.join(json_normalize(df[0].to_list()))\\\n",
    "       .drop([0], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. DateTime Format:\n",
    "\n",
    " Datetime data is in an Epoch format, in order to be human readable we need to transform it into a datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time was in Epoch format so we need to change it into datetime\n",
    "\n",
    "import datetime\n",
    "\n",
    "df['dt'] = pd.to_datetime(df['dt'], unit='s')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sinking weather into Data Lake:\n",
    "\n",
    "The sink is being performed into parquet format and compressed in snappy in order to optimize performance and the storage use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sinking the clean data into the datalake, partitioned by city\n",
    "\n",
    "df.to_parquet('C:/Users/E.ALVAREZHERNANDEZ/VS Projects/caseStudy/clean_data/weather/',compression='snappy', partition_cols='city')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Delta weather table:\n",
    "\n",
    "As we recreate the data from the datalake parquet files the processing times improves, from 56 secs to 0.1 secs and the storage use from 2.26 MB to 320 KB, it reduces the size by 84%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pulling clean weather temperature\n",
    "\n",
    "cweather = pd.read_parquet('C:/Users/E.ALVAREZHERNANDEZ/VS Projects/caseStudy/clean_data/weather')\n",
    "\n",
    "cweather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to get the avg temperature I will subset\n",
    "\n",
    "temperature = cweather[['dt', 'temp', 'city']]\n",
    "\n",
    "temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Connecting with shipments DB:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import logging\n",
    "\n",
    "conn = psycopg2.connect(host='localhost',\n",
    "                        database='bttf',\n",
    "                        user='postgres',\n",
    "                        password='Zurich2022!',\n",
    "                        port='5432')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = conn.cursor()\n",
    "\n",
    "pointer.execute(\"SELECT * FROM shipments.shipments\")\n",
    "\n",
    "rows = pointer.fetchall() #Tupple format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shdf = pd.DataFrame(rows, columns=['id', 'truck', 'driver', 'shipment_start_timestamp', 'shipment_end_timestamp', 'start_location', 'end_location', 'shipment_distance', 'consumed_fuel'])\n",
    "shdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Fixing shipments timestamp formats:\n",
    "\n",
    "Asumption: Weather doesn't have a big variance from hour to hour, so to facilitate the analysis I'm going to round the shipment hours to the closest hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dates are in sql format, so in here I invert them to match the df convention\n",
    "\n",
    "import datetime\n",
    "\n",
    "\n",
    "shdf['shipment_start_timestamp'] = pd.to_datetime(pd.to_datetime(shdf['shipment_start_timestamp'] ).apply(lambda x: datetime.datetime.strftime(x, '%Y-%m-%d %H:%M:%S')))\n",
    "shdf['shipment_end_timestamp'] = pd.to_datetime(pd.to_datetime(shdf['shipment_end_timestamp'] ).apply(lambda x: datetime.datetime.strftime(x, '%Y-%m-%d %H:%M:%S')))\n",
    "\n",
    "shdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shdf['shipment_start_timestamp'] = shdf['shipment_start_timestamp'].dt.round('H')\n",
    "shdf['shipment_end_timestamp'] = shdf['shipment_end_timestamp'].dt.round('H')\n",
    "\n",
    "#Calculates how many liters the trucks consume every 100 kms\n",
    "shdf['fConsumption']= (shdf['consumed_fuel']*100)/shdf['shipment_distance']\n",
    "shdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Merging data:\n",
    "\n",
    "In this step we merge the dataframes to get the weather conditions at location and time for the shipments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging to get the first temperature matching the start location and the start timestamp\n",
    "\n",
    "\n",
    "dfmerge = pd.merge(shdf, temperature, how='left', left_on=[shdf['start_location'].astype(str), shdf['shipment_start_timestamp'].astype(str)], right_on=[temperature['city'].astype(str), temperature['dt'].astype(str)])\n",
    "dfmerge= dfmerge.rename(columns={'temp':'startTemp'}).drop(['key_0', 'key_1', 'city', 'dt'], axis=1)\n",
    "\n",
    "dfmerge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging to get the end temperature matching the end location and the end timestamp\n",
    "\n",
    "\n",
    "dfmerge2 = pd.merge(dfmerge, temperature, how='left', left_on=[dfmerge['end_location'].astype(str), dfmerge['shipment_end_timestamp'].astype(str)], right_on=[temperature['city'].astype(str), temperature['dt'].astype(str)])\n",
    "dfmerge2= dfmerge2.drop(['key_0', 'key_1', 'dt', 'city'], axis=1).rename(columns={'temp':'endTemp'})\n",
    "dfmerge2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the average temperature for the trip, and setting the timestamp to datetime as it was required to pass them as strings for the merge\n",
    "\n",
    "shipment = dfmerge2\n",
    "\n",
    "shipment['avgTemp']= shipment[['startTemp','endTemp']].mean(axis=1)\n",
    "\n",
    "\n",
    "shipment['shipment_start_timestamp'] = pd.to_datetime(shipment['shipment_start_timestamp'])\n",
    "shipment['shipment_end_timestamp'] = pd.to_datetime(shipment['shipment_end_timestamp'])\n",
    "\n",
    "shipment['duration'] = (shipment['shipment_end_timestamp'] - shipment['shipment_start_timestamp']).astype('timedelta64[h]')\n",
    "\n",
    "shipment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The correlation matrix shows some results already\n",
    "\n",
    "\n",
    "shipment.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sinking the clean data into the datalake\n",
    "\n",
    "\n",
    "shipment.to_parquet('C:/Users/E.ALVAREZHERNANDEZ/VS Projects/caseStudy/clean_data/DataLake/gold.parquet',compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Plotting the correlation:\n",
    "\n",
    "Since the weather temperature seems to be fake data, f.e: Barcelona couldn't be -12Celsius in July, the correlation is cohorsed and next I'm plotting the the relation to make it visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "analysis = pd.read_parquet('C:/Users/E.ALVAREZHERNANDEZ/VS Projects/caseStudy/clean_data/DataLake/gold.parquet')\n",
    "\n",
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = analysis[analysis['start_location']=='Barcelona']\n",
    "analysis= analysis[['fConsumption', 'avgTemp']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.plot(kind='scatter',x='avgTemp', y='fConsumption')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "analysis = analysis.dropna(subset=['avgTemp'])\n",
    "\n",
    "x = analysis['avgTemp']\n",
    "y = analysis['fConsumption']\n",
    "\n",
    "\n",
    "\n",
    "scipy.stats.pearsonr(x,y)\n",
    "\n",
    "scipy.stats.spearmanr(x,y)\n",
    "\n",
    "scipy.stats.kendalltau(x,y)\n",
    "\n",
    "result = scipy.stats.linregress(x,y)\n",
    "result.slope\n",
    "result.intercept\n",
    "result.rvalue\n",
    "result.pvalue\n",
    "result.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "slope, intercept, r, p, stderr = scipy.stats.linregress(x,y)\n",
    "\n",
    "line = f'Regression line: y:{intercept:.2f}+{slope:.2f}x, r={r:.2f}'\n",
    "\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, linewidth=0, marker='s', label='Data points')\n",
    "ax.plot(x, intercept + slope*x, label=line)\n",
    "ax.set_xlabel('Avg Temperature')\n",
    "ax.set_ylabel('Fuel Consumption')\n",
    "ax.legend(facecolor='white')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4459300ce7937238969313270312f316576674cb97307ebd928dfd113555c735"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
